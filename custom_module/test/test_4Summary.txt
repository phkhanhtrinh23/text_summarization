 We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo localization. Depthwise Separable Convolution The MobileNet model is based on depthwise separable convolutions which is a form of factorized convolutions which factorize a standard convolution into a depthwise convolution and a 1 x 1 convolution called a pointwise convolution. A standard convolution both filters and combines inputs into a new set of outputs in one step. A standard convolutional layer takes as input a Dp x Dy Xx M feature map F and produces a Dr x Dr x N feature map G where Dp is the spatial width and height of a square input feature map', M is the number of input channels (input depth), Dg is the spatial width and height of a Square output feature map and WN is the number of output channel (output depth). The standard convolutional layer is parameterized by convolution kernel K of size Dy x Dx x M x N where Dx is the spatial dimension of the kernel assumed to be square and   is number of input channels and N is the number of output channels as defined previously. The output feature map for standard convolution assuming stride one and padding is computed as: Grin   ) Ki jamin   Fe4i 1,14j 1,m (1) Ew Standard convolutions have the computational cost of: De De M N Dp Dp (2) where the computational cost depends multiplicatively on the number of input channels MM, the number of output channels N the kernel size Dy, x D x and the feature map size Dr  x Dr. MobileNet models address each of these terms and their interactions. The standard convolution operation has the effect of filtering features based on the convolutional kernels and combining features in order to produce a new representation. Depthwise convolution with one filter per input channel (input depth) can be written as: Grim   SS Ki,jm   Peti—1i 45— 1,m (3) oy) where K is the depthwise convolutional kernel of size Dr X Dr X M where the m;,, filter in K is applied to the mz; channel in F to produce the m:, channel of the filtered output feature map G. Depthwise convolution has a computational cost of: De De: M Dr  Dr (4) Depthwise convolution is extremely efficient relative to standard convolution. MobileNet uses 3 x 3 depthwise separable convolutions which uses between 8 to 9 times less computation than standard convolutions at only a small reduction in accuracy as seen in Section 4. The standard convolutional filters in (a) are replaced by two layers: depthwise convolution in (b) and pointwise convolution in (c) to build a depthwise separable filter. We can now express the computational cost for the core layers of our network as depthwise separable convolutions with width multiplier a and resolution multiplier p: De: Dr aM pDpr: pDr aM aN: pDr:pDFr (7) where p   (0,1  which is typically set implicitly so that the input resolution of the network is 224, 192, 160 or 128. p   11s the baseline MobileNet and p   1 are reduced computation MobileNets. Depthwise Separable vs Full Convolution MobileNet Model ImageNet Million Million Accuracy Mult Adds Parameters Conv MobileNet 71.7  4866 29.3 MobileNet 70.6  569 4.2 Table 5. We then demonstrated how to build smaller and faster MobileNets using width multiplier and resolution multiplier by trading off a reasonable amount of accuracy to reduce size and latency. arXiv preprint arXiv: 1610.02357v2, 2016. arXiv preprint arXiv: 1512.03385, 2015. arXiv preprint arXiv: 1503.02531, 2015. arXiv preprint arXiv: 1611.10012, 2016. arXiv preprint arXiv: 1502.03167, 2015. arXiv preprint arXiv: 1405.3866, 2014. arXiv preprint arXiv: 1408.5093, 2014. arXiv preprint arXiv: 1412.5474, 2014. arXiv preprint arXiv: 1412.6553, 2014. arXiv preprint arXiv: 1512.02325, 2015. arXiv preprint arXiv: 1603.05279, 2016. arXiv preprint arXiv: 1409. arXiv preprint arXiv: 1602.07261, 2016. arXiv preprint arXiv: 1512.00567, 2015. arXiv preprint arXiv: 1608.04337, 2016.